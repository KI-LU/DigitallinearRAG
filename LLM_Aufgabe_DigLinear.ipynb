{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd440299-0562-4744-9fa3-acf638f097a0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u7mAAYYvtYLE",
    "outputId": "3851bbc1-bd23-4a53-a400-d438784e94f5"
   },
   "source": [
    "# Modul 08 Large Language Models und Retrieval Augmented Generation\n",
    "\n",
    "In diesem Notebook lernst du zuerste unterschiedliche Anwendungsfälle von LLMs kennen. Anschließend wirst du für ein vorgegebenes Szenario selbst einen Wartungsassistenten implementieren. \n",
    "\n",
    "## Was ist der Anwendungsfall?\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Bevor es mit der Umsetztung losgeht, machst du dir immer Gedanken darüber, welches <i>Problem </i> du lösen möchtest. </b> Aus deinem Problem ergibt sich nämlich welche Technik die richtige ist, um dieses zu lösen. Denn: Es gibt viele Arten von KI-Algorithmen und noch mehr Arten diese anzuwenden. Nicht jeder Algorithmus bzw. jede Technik passt zu jedem Problem! </div>\n",
    "\n",
    "Damit die Anwendung von LLMs Sinn macht, muss das Problem zunächst mit **Text** zu tun haben. Doch dann stellen sich weitere Fragen. Denn es gibt viele Arten von LLMs die auch unterschiedlich und eingesetzt werden. Einige davon haben wir auf den folgenden Karteikarten beschrieben.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-Info\">\n",
    "<b> QUIZ: </b> Führe die nächste Codezelle aus. Dann werden dir digitale Karteikarten angezeigt. Durch anklicken der Karte dreht sich sich auf die andere Seite. Mit einem Klick auf das Feld <i>Next</i> (unten rechts) kannst du zur nächsten Karte wechseln."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1d21e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install jupyterquiz\n",
    "from jupytercards import display_flashcards\n",
    "from helpers.fc_helpers import load_flashcards\n",
    "\n",
    "fc = load_flashcards('fc')\n",
    "display_flashcards(fc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886bb268",
   "metadata": {},
   "source": [
    "Du weißt bereits, dass Neuronale Netze generell **sehr lang** zum Training brauchen und, dass LLMs auch meistens nicht 'online' lernen. Sie werden in einer gewissen Phase trainiert und machen dann nur noch Vorhersagen. \n",
    "\n",
    "D.h. ein LLM mit Chatbot fine-tuning beantwortet sämtliche Anfragen aus seinen eigenen 'Gewichten' heraus. Diese sind durch das Training eingestellt, spiegeln also in gewisser Weise statistische Regelhaftigkeiten in den Trainingsdaten wieder. \n",
    "\n",
    "<img src=\"pictures/LLM_diagramm.png\" alt=\"Platzhalter, Bild kann nicht angezeigt werden.\">\n",
    "\n",
    "Für viele Anwendungen ist das kein Problem. Obwohl sich Sprache ständig verändert, geschieht das nicht so schnell, dass die Modelle nach wenigen Monaten unbrauchbar werden. Für andere Anwendungen ist es allerdings ungünstig."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c64476a",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "**Nehmen wir folgendes Beispiel:**\n",
    "\n",
    "Du willst wissen, wer der Premierminister von Armenien ist & dein Onkel hat zu Armenien mal einen Kurs besucht. Also stellst du ihm die Frage: *\"Wer ist Premierminister von Armenien?\"* Er antwortet: *\"Andranik Markarjan von der republikanischen Partei ist der Premier von Armenien.\"* Die Antwort klingt überzeugend und plausibel, vor allem, wenn man selbst keine Ahnung von diesem Land hat. \n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Frage:</b> Woher weiß ich wie verlässlich die Antwort ist? Wie lässt sich diese Information überprüfen? Überlegt gemeinsam und macht Notizen! </div>\n",
    "\n",
    "\n",
    "Eine einfache online Suche zeigt, wo der Fehler liegt. Die Schulzeit des Onkels ist schon 20 Jahr her, zu dieser Zeit hat er zuletzt etwas über armenische Politiker gelernt. Damals war Andranik Markarjan wirklich Premierminister. Aber inzwischen stimmt das nicht mehr. Denn, wer Premierminister oder Kanzlerin eines Landes ist, ändert sich alle paar Jahre. Aus den Namen der früheren Präsidenten lässt sich nicht auf verlässlich auf den aktuellen Namen schließen.\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Achtung:</b> Verwechslungsgefahr! Weil LLMs meistens schön formulierte und präzise klingende Antworten auf unsere Fragen erstellen, verwenden viele Nutzende sie als Suchmaschinen. </div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b3af56",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-Info\">\n",
    "<b>QUIZ:</b> Führe die nächste Codezelle aus, um die Quizfrage anzuzeigen. </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06db3166",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from helpers.q_helpers import load_question\n",
    "from jupyterquiz import display_quiz\n",
    "q1 = load_question('q1')\n",
    "display_quiz([q1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6262c0c2",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation (RAG)\n",
    "\n",
    "Retrieval Augmented Generation (kurz: RAG, Deutsch: abruf-erweiterte Generation) ist eine Technik zur Verwendung von Large Language Models (LLMs). Die Anwendungsbeispiele für RAG sind sehr vielfältig. Grundsätzlich geht es darum, einem LLM spezielles Wissen bereitzustellen. \n",
    "Wir schaffen einen Mechanismus der in einer Textdatenbank nachschlagen kann. Die Ergebnisse dieser Suche werden dann an unser LLM übergeben. Das macht aus den gefundenen Textschnipseln eine wohlformulierte Antwort. \n",
    "\n",
    "![Platzhalter](pictures/RAG_LLM_diagramm.png)\n",
    "\n",
    "So muss das LLM mit dem wir arbeiten nicht ständig nachtrainiert werden. Trotzdem kann neue Information über eine Datenbank verlässlich und elegant in den Generationsprozess miteinbezogen werden. In der Datenbank kann z.B. veraltete Information genauso problemlos gelöscht werden, wie neue Information hinzugefügt werden kann.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eaa6d26",
   "metadata": {},
   "source": [
    "## 1 Unser Szenario \n",
    "\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> <b> Das Unternehmen Alstom produziert Straßenbahnen. In der Produktionslinie B soll ein \"Intelligenter Wartungsassistent\" implementiert werden.\n",
    "Dieser Assistent soll Technikern bei der Fehlerdiagnose und Wartung von Fertigungsrobotern und Montageanlagen helfen. Die Techniker geben ihre Beobachtung, oder Fehlercodes in ein Chatfenster ein und der Wartungsassistent gibt Ihnen eine Handlungsempfehlung.</b> Diesen Wartungsassistenten wirst du im Laufe dieses Notebooks implementieren. </div>\n",
    "\n",
    "![Platzhalter](pictures/Produktionslinie.jpg)\n",
    "\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-Info\">\n",
    "<b>QUIZ:</b> Führe die nächste Codezelle aus, um die Quizfrage anzuzeigen. </div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1959c6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "q2 = load_question('q2')\n",
    "display_quiz([q2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fd2a0e-3555-4a80-ba97-10f704ac8410",
   "metadata": {},
   "source": [
    "## 2.1 Installieren von Software-Paketen\n",
    "\n",
    "Der erste Schritt beim Implementieren ist immer das Installieren der richtigen Software-Pakete (auch Bibliotheken genannt). \n",
    "\n",
    "Die Bibliothek, die wir für unser RAG-Modul verwenden, heißt <strong>llama-index </strong>. Die Dokumentation der Bibliothek findest du hier:  <a> https://docs.llamaindex.ai/en/stable/ </a>. Für die Arbeit mit dem Notebook während der Praxistage benötigst du die Dokumentation allerdings nicht."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cdc1d5-716a-48c0-bc0d-98d0569f9a46",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Info:</b> Dies ist ein einmaliger Schritt. D.h er muss nur ausgeführt werden, wenn das Notebook auf einem neuen Computer läuft. Du kannst also direkt zum Importieren von Funktionen übergehen. </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ec6e12-e2a8-436b-ac3d-1bc1832f4a28",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uFwm7wDSoF3V",
    "outputId": "991ac34d-4f12-440b-968a-a2e23df797ec"
   },
   "outputs": [],
   "source": [
    "#!pip install llama-index\n",
    "#!pip install llama-index-embeddings-huggingface\n",
    "#!pip install llama-index-llms-ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3aadaeb-e923-44a9-b18c-c227a4b63a1e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u7mAAYYvtYLE",
    "outputId": "3851bbc1-bd23-4a53-a400-d438784e94f5"
   },
   "source": [
    "## 2.2 Importieren von Funktionen\n",
    "\n",
    " \n",
    "Um einzelne Funktionen der Bibliothek nutzen zu können, müssen wir die benötigten Elemente von <strong>llama-index </strong> importieren. Die 5 Elemente <i> Settings, VectorStoreIndex, SimpleDirectoryReader, HuggingFaceEmbedding und ollama </i> werden wir im Laufe des Notebooks für unseren RAG-Wartungsassistenten verwenden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3f7f94-6566-4288-8cae-d74169fac6a0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u7mAAYYvtYLE",
    "outputId": "3851bbc1-bd23-4a53-a400-d438784e94f5"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Auftrag:</b> Führe die nächste Codezelle aus, um die benötigten Bibliotheken zu importieren.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f59ca59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings, VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.ollama import Ollama\n",
    "#from llama_index.core.callbacks import CallbackManager, LlamaDebugHandler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84a9ef2",
   "metadata": {},
   "source": [
    "## 3.1 Suche nach Relevanten Daten\n",
    "\n",
    "Jetzt geht es darum, die Wissensbasis zu kuratieren, also relevante Texte zu finden und sie dem Modell zugänglich zu machen. Denn unser RAG- Modul besteht aus zwei Teilen: einer Vektordatenbank aka Wissensbasis und einer Suchfunktion. \n",
    "\n",
    "\n",
    "![Platzhalter](pictures/RAG_Modul.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368baa92",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Auftrag:</b> Überlege für ca. 5 Minuten, welche Dokumente für den RAG-Wartungsassistenten sinnvoll oder notwendig wären! Notiert eure Überlegungen in dem Text Dokument, welches schon geöffnet ist. </div>\n",
    "\n",
    "Normalerweise würdest du dich nach diesen Überlegungen nun selbst darum bemühen, die richtigen Dokumente zu finden und am richtigen Ort zu speichern. In diesem Szenario haben wir das schon erledigt. Stattdessen kommt ein Quiz zu Problemen, die beim Befüllen der Wissensbasis auftreten können.\n",
    "\n",
    "<div class=\"alert alert-block alert-Info\">\n",
    "<b>QUIZ:</b> Führe die nächste Codezelle aus, um die Quizfrage anzuzeigen. </div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f2142a",
   "metadata": {},
   "outputs": [],
   "source": [
    "q4 = load_question('q4')\n",
    "q5 = load_question('q5')\n",
    "display_quiz([q4, q5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde2b65f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Der Ordner <i> Wissensbasis </i> muss außerdem am richtigen Ort sein, damit unser RAG-System darauf zugreifen kann. \n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Auftrag:</b> Auf Moodle in dem GenAI Modul gibt es zwei Dokumente: Notfallplan_Stromausfall und Werkstoffe_Materialkunde. Seh dir für <b> ca. 5 Minuten </b> die zwei Dokumente an. Später werden wir dem Wartungsassistent die zwei Fragen anhand dieser Dokumenten stellen:\n",
    "\n",
    "1. Welche Materialen können in der Schienenfahrzeugproduktion verwendet werden?\n",
    "\n",
    "2. Wer sind bei einem Stromausfall verantwortlich?  </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904afef9",
   "metadata": {},
   "source": [
    "## 3.2 Einlesen der Daten\n",
    "Mit der nächsten Codezeile erhält das System Zugriff auf die Dateien welche in dem Ordner <i> Wissensbasis </i> gespeichert sind. Alle Dateien in diesem Ordner werden mit dem Objekt **documents** gleichgesetzt. So kann es anderen Funktionen zur Verfügung gestellt werden.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Auftrag:</b> Führe die nächste Codezelle aus. </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3fba09",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(\"Wissensbasis\").load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2928b58-063d-4101-a82c-f607c0d5e145",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-Info\">\n",
    "<b>QUIZ:</b> Führe die nächste Codezelle aus, um die Quizfrage anzuzeigen. Erinner dich an das Video aus dem Vorkurs: </div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d6e5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "q3 = load_question('q3')\n",
    "display_quiz([q3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237fad56",
   "metadata": {},
   "source": [
    "\n",
    "## 3.3 Embedding\n",
    "\n",
    "Eben haben wir sichergestellt, dass die Dokumente in unserer Wissensbasis durchsuchbar sind. Das heißt eine Suchfunktion kann Wörter und Sätze aus den Dokumenten entnehmen. Trotzdem liegen die Texte darin noch nicht im richtigen Format für ein LLM vor.<strong> Denn Neuronale Netze brauchen numerischen Input! </strong> Wir müssen also noch einen Umwandlungsschritt machen: Jedes einzele enthaltene Wort muss in einen Vektor umgewandelt werden. \n",
    "\n",
    "![Platzhalter](../RAG/pictures/Embedding.png)\n",
    "\n",
    "Dafür werden bereits trainierte Embedding Modelle verwendet. Die meisten Embedding Modelle sind einsprachig. Einige große Modelle sind mehrsprachig, können aber trotzdem meistens eine besonders gut. \n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Auftrag:</b> Führe die nächste Codezelle aus, um das ein <b> Embedding-Modell </b> von der Plattform Huggingface zu importieren. </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07af3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"mixedbread-ai/deepset-mxbai-embed-de-large-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3625ea",
   "metadata": {},
   "source": [
    "## 3.4 Speichern in Vektordatenbank\n",
    "\n",
    "Nachdem das Embeddingmodell geladen wurde, kann es auf den Text in unserem **documents** Objekt angewendet werden. \n",
    "Mit der nächsten Codezeile passiert aber noch etwas mehr:\n",
    "\n",
    "<ul>\n",
    "<li> Jedes Wort wird ein Vektor zugeordnet (wie in der Abbildung oben) </li>\n",
    "<li> Der ganze Text wird in kleine Schnipsel (Chunks) aufgeteilt. Default: 1024 Zeichen pro Chunk </li>\n",
    "<li> Die Chunks werden dann wieder in einem Vektorraum (VektorStoreIndex) gespeichert. Dadurch sind sie für die Suchfunktion, die wir gleich zusammensatzen, leichter durchsuchbar. </li>\n",
    "</ul>\n",
    "\n",
    "Grob können wir uns also vorstellen, dass jedes Chunk einen Punkt im Vektorraum zugeordnet bekommt. Auf diese Weise können die Chunks später mit Anfragen an das Modell verglichen werden.\n",
    "\n",
    "![Platzhalter](pictures/Embedding_chunks.png)\n",
    "\n",
    " Hier ein Beispiel, wie die Suchfunktion ein Mapping zwischen einer Useranfrage und den Chunks der Wissensdatenbank herstellt:\n",
    " Wenn ein User eine Frage stellt, werden die einzelnen Worte auch durch das Embedding Modell in Vektoren umgewandelt. Die ganze Frage wird an einem Ort in der *VectorDataBase* platziert. Der Stern repräsentiert hier die Useranfrage. Durch den K-Nächste-Nachbarn Algorithmus (KNN) wird bestimmt welche Chunks aus der Datenbank abgefragt werden. Im Bild durch den Kreis gezeigt, siehst du welche Chunks bei k = 3 ausgewählt werden.\n",
    "\n",
    "![Platzhalter](pictures/Embedding_search_.png)\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Auftrag: </b>Führe die nächste Codezelle aus, um die Vektordatenbak zu befüllen.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c09807",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62c66a2",
   "metadata": {},
   "source": [
    "## 4 Importieren des LLMs\n",
    "\n",
    "In diesem Schritt geht es um die Auswahl und das Laden des passenden LLMs. Auch hier gibt es eigentlich wieder eine Vielzahl an Möglichkeiten, denn es gibt mittlerweile eine große Auswahl an Modellen. Das richtige Modell zu finden ist allerdings garnicht so leicht. Deshalb ist hier schon eines für dich vorausgewählt, um nicht von der Internetverbindung anhänging zu sein und aus Datenschutzgründen wollten wir zunächst mal ein lokales Modell haben. D.h. es läuft direkt auf dem Rechner an dem du gerade arbeitest.\n",
    "Wichtige Auswahlkriterien sind: \n",
    "<ul>\n",
    "    <li> Es muss Deutsch sprechen. </li>\n",
    "    <li> Es muss auf Frage-Antwort Szenarien trainiert sein. </li>\n",
    "    <li> Es muss klein genug sein, um auf unseren Rechnern laufen zu können. </li>\n",
    "    <li> Es muss Open Source verfügbar sein.</li>\n",
    "</ul>\n",
    "\n",
    "Nach langer Recherche und viel Ausprobieren ist hier die Entscheidung auf das Modell <i> phi3 </i> gefallen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c40975",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Auftrag:</b> Führe die nächste codezelle aus, um das LLM zu laden. </div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97224d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.llm = Ollama(model=\"phi3\", request_timeout=360.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14067d3d-62f0-4784-8f15-7bd304cf8252",
   "metadata": {},
   "source": [
    "## 5 Zusammensetzen, Ausprobieren, Evaluieren\n",
    "\n",
    "Die Nächste Codezeile setzt nun alle Elemente, die du bisher vorbereitet hast, zusammen. D.h.: Es ist Zeit zum Ausprobieren! \n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Auftrag:</b> Überlege dir nun eine Anfrage an den RAG-Wartungsassistenten, oder wähle eine der Fragen, die du dir vorhin notiert hast. <b> Wenn du die nächste Codezelle ausfüllst, sieh an den oberen Bildschirmrand. Dort erscheint ein Feld, indem du die Frage eingeben kannst! </b> Bestätige mit Enter, um die Frage abzuschicken. </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4e3824",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.query_helpers import set_query\n",
    "TASK = 'WRITE'\n",
    "\n",
    "\n",
    "#debug_handler = LlamaDebugHandler()\n",
    "#callback_manager = CallbackManager([debug_handler])\n",
    "query_engine = index.as_query_engine(\n",
    "    #callback_manager = callback_manager\n",
    ")\n",
    "\n",
    "response = query_engine.query(set_query(TASK))\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8e319e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video\n",
    "Video(\"Fragen_RAG.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4843f598",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-success\"> Lese dir die Antwort des RAG-Wartungsassitenten <b> genau </b> durch, kopiere sie in das Text Dokument zu deiner Frage und beurteile die Qualität der Antwort! Notiere dir auch, anhand welcher Kriterien du die Qualität der Antworten beurteilt hast. \n",
    "</div>\n",
    "\n",
    "## 6 Gegenprobe\n",
    "\n",
    "Das Modell **Phi3** kannst du auch ohne RAG-Modul im Terminal verwenden. Wenn das Modell noch nicht aktiviert ist, sieht der Terminal so aus:\n",
    "\n",
    "![Platzhalter](pictures/Terminal_rag.png)\n",
    "\n",
    "Um es zu starten, musst du noch den Befehl *ollama run phi3* eintippen und mit *Enter* bestätigen.\n",
    "Danach sieht der Terminal so aus und kannst direkt chatten.\n",
    "\n",
    "![Platzhalter](pictures/Terminal_phi3_update.png)\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Aufgabe:</b> Stelle die selbe Frage, die du eben an den RAG-Assistenten gestellt hast, direkt an Phi3. Vergleiche die Antworten anhand folgender Kriterien:\n",
    "</div>\n",
    "\n",
    "<ul>\n",
    "<li>Wird die Frage richtig beantwortet?</li>\n",
    "<li>Wie präzise sind die Antworten?</li>\n",
    "</ul>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b113d1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Video(\"Fragen_phi3.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b31700",
   "metadata": {},
   "source": [
    "<div style=\"display:none\" >\n",
    "    print(\"\\nRetrieved Chunks:\")\n",
    "    for i, node in enumerate(debug_handler.get_retrived_nodes()):\n",
    "    print(f\"\\n--- Chunk {i+1} ---\")\n",
    "    print(f\"Text: {node.text}\")\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".ragenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
